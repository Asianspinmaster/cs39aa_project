{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f24d802",
   "metadata": {
    "papermill": {
     "duration": 0.009514,
     "end_time": "2022-11-17T23:08:30.202266",
     "exception": false,
     "start_time": "2022-11-17T23:08:30.192752",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CS 39aa Final Project Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a49b15",
   "metadata": {
    "papermill": {
     "duration": 0.007795,
     "end_time": "2022-11-17T23:08:30.218482",
     "exception": false,
     "start_time": "2022-11-17T23:08:30.210687",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Introduction/Background\n",
    "_This project is based off a data set given by Kaggle \"Coronavirus tweets NLP - Text Classification.\" These tweets have been pulled from twitter and categorized accordingly such as the date, location, original tweet message, and sentiment. The data was given in two different sets of data. One data set as the training for training your data, and the other as the testing for testing and validating the data. This problem is a multi-class text classification problem meaning that the goal of this project is to project whether a certain tweet within the dataset is a \"positive\", \"negative\", \"extremely positive\", \"extremely negative\", or \"neutral\" tweet based on the words that were used within the tweet._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b028ab7",
   "metadata": {
    "papermill": {
     "duration": 0.008521,
     "end_time": "2022-11-17T23:08:30.235560",
     "exception": false,
     "start_time": "2022-11-17T23:08:30.227039",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4fbc46d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T23:08:30.254187Z",
     "iopub.status.busy": "2022-11-17T23:08:30.253562Z",
     "iopub.status.idle": "2022-11-17T23:08:30.616361Z",
     "shell.execute_reply": "2022-11-17T23:08:30.615343Z"
    },
    "papermill": {
     "duration": 0.375993,
     "end_time": "2022-11-17T23:08:30.619792",
     "exception": false,
     "start_time": "2022-11-17T23:08:30.243799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>44953</td>\n",
       "      <td>NYC</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>44954</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>44955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>44956</td>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>44957</td>\n",
       "      <td>Melbourne, Victoria</td>\n",
       "      <td>03-03-2020</td>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName             Location     TweetAt  \\\n",
       "0         1       44953                  NYC  02-03-2020   \n",
       "1         2       44954          Seattle, WA  02-03-2020   \n",
       "2         3       44955                  NaN  02-03-2020   \n",
       "3         4       44956          Chicagoland  02-03-2020   \n",
       "4         5       44957  Melbourne, Victoria  03-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  TRENDING: New Yorkers encounter empty supermar...  Extremely Negative  \n",
       "1  When I couldn't find hand sanitizer at Fred Me...            Positive  \n",
       "2  Find out how you can protect yourself and love...  Extremely Positive  \n",
       "3  #Panic buying hits #NewYork City as anxious sh...            Negative  \n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...             Neutral  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_data = pd.read_csv('../input/covid-19-nlp-text-classification/Corona_NLP_test.csv')\n",
    "train_data = pd.read_csv('../input/covid-19-nlp-text-classification/Corona_NLP_train.csv', encoding= 'ISO-8859-1')\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe8b92b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T23:08:30.638894Z",
     "iopub.status.busy": "2022-11-17T23:08:30.638069Z",
     "iopub.status.idle": "2022-11-17T23:08:30.650319Z",
     "shell.execute_reply": "2022-11-17T23:08:30.649287Z"
    },
    "papermill": {
     "duration": 0.024312,
     "end_time": "2022-11-17T23:08:30.652664",
     "exception": false,
     "start_time": "2022-11-17T23:08:30.628352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName   Location     TweetAt  \\\n",
       "0      3799       48751     London  16-03-2020   \n",
       "1      3800       48752         UK  16-03-2020   \n",
       "2      3801       48753  Vagabonds  16-03-2020   \n",
       "3      3802       48754        NaN  16-03-2020   \n",
       "4      3803       48755        NaN  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
       "1  advice Talk to your neighbours family to excha...            Positive  \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive  \n",
       "3  My food stock is not the only one which is emp...            Positive  \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7192e560",
   "metadata": {
    "papermill": {
     "duration": 0.008301,
     "end_time": "2022-11-17T23:08:30.671164",
     "exception": false,
     "start_time": "2022-11-17T23:08:30.662863",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here we drop the username, screenname, location, and tweetat columns because they hold no value in trying to solve the sentiment of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a24f7a6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T23:08:30.690554Z",
     "iopub.status.busy": "2022-11-17T23:08:30.689834Z",
     "iopub.status.idle": "2022-11-17T23:08:30.703592Z",
     "shell.execute_reply": "2022-11-17T23:08:30.702780Z"
    },
    "papermill": {
     "duration": 0.026204,
     "end_time": "2022-11-17T23:08:30.706035",
     "exception": false,
     "start_time": "2022-11-17T23:08:30.679831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data.drop(columns = ['UserName','ScreenName','Location','TweetAt'], inplace=True)\n",
    "test_data.drop(columns = ['UserName','ScreenName','Location','TweetAt'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e19df5f",
   "metadata": {
    "papermill": {
     "duration": 0.00888,
     "end_time": "2022-11-17T23:08:30.724063",
     "exception": false,
     "start_time": "2022-11-17T23:08:30.715183",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We find the sentiment precentage of the training and testing data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9efd733",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T23:08:30.746672Z",
     "iopub.status.busy": "2022-11-17T23:08:30.745946Z",
     "iopub.status.idle": "2022-11-17T23:08:30.758108Z",
     "shell.execute_reply": "2022-11-17T23:08:30.756974Z"
    },
    "papermill": {
     "duration": 0.024416,
     "end_time": "2022-11-17T23:08:30.760394",
     "exception": false,
     "start_time": "2022-11-17T23:08:30.735978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Negative              0.274092\n",
       "Positive              0.249342\n",
       "Neutral               0.162981\n",
       "Extremely Positive    0.157715\n",
       "Extremely Negative    0.155872\n",
       "Name: Sentiment, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.Sentiment.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee16dcd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T23:08:30.780004Z",
     "iopub.status.busy": "2022-11-17T23:08:30.779210Z",
     "iopub.status.idle": "2022-11-17T23:08:30.789895Z",
     "shell.execute_reply": "2022-11-17T23:08:30.788934Z"
    },
    "papermill": {
     "duration": 0.022965,
     "end_time": "2022-11-17T23:08:30.792089",
     "exception": false,
     "start_time": "2022-11-17T23:08:30.769124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive              0.277523\n",
       "Negative              0.240955\n",
       "Neutral               0.187404\n",
       "Extremely Positive    0.160945\n",
       "Extremely Negative    0.133173\n",
       "Name: Sentiment, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.Sentiment.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "641e62f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T23:08:30.811415Z",
     "iopub.status.busy": "2022-11-17T23:08:30.811019Z",
     "iopub.status.idle": "2022-11-17T23:08:30.817768Z",
     "shell.execute_reply": "2022-11-17T23:08:30.816672Z"
    },
    "papermill": {
     "duration": 0.019218,
     "end_time": "2022-11-17T23:08:30.820122",
     "exception": false,
     "start_time": "2022-11-17T23:08:30.800904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41157, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd99c4bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T23:08:30.843645Z",
     "iopub.status.busy": "2022-11-17T23:08:30.842897Z",
     "iopub.status.idle": "2022-11-17T23:08:30.849500Z",
     "shell.execute_reply": "2022-11-17T23:08:30.848651Z"
    },
    "papermill": {
     "duration": 0.019176,
     "end_time": "2022-11-17T23:08:30.851677",
     "exception": false,
     "start_time": "2022-11-17T23:08:30.832501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3798, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e086e915",
   "metadata": {
    "papermill": {
     "duration": 0.008692,
     "end_time": "2022-11-17T23:08:30.869421",
     "exception": false,
     "start_time": "2022-11-17T23:08:30.860729",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We tokenize each word of the tweet adding it as a new column to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ae65239",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T23:08:30.889328Z",
     "iopub.status.busy": "2022-11-17T23:08:30.888571Z",
     "iopub.status.idle": "2022-11-17T23:08:39.641724Z",
     "shell.execute_reply": "2022-11-17T23:08:39.640509Z"
    },
    "papermill": {
     "duration": 8.765689,
     "end_time": "2022-11-17T23:08:39.643988",
     "exception": false,
     "start_time": "2022-11-17T23:08:30.878299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>tokens_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[@menyrbie, @phil_gahan, @chrisitv, https://t....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[advice, talk, to, your, neighbours, family, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[coronavirus, australia, :, woolworths, to, gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[my, food, stock, is, not, the, only, one, whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>[me, ,, ready, to, go, at, supermarket, during...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment  \\\n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral   \n",
       "1  advice Talk to your neighbours family to excha...            Positive   \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive   \n",
       "3  My food stock is not the only one which is emp...            Positive   \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative   \n",
       "\n",
       "                                          tokens_raw  \n",
       "0  [@menyrbie, @phil_gahan, @chrisitv, https://t....  \n",
       "1  [advice, talk, to, your, neighbours, family, t...  \n",
       "2  [coronavirus, australia, :, woolworths, to, gi...  \n",
       "3  [my, food, stock, is, not, the, only, one, whi...  \n",
       "4  [me, ,, ready, to, go, at, supermarket, during...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tk = TweetTokenizer()\n",
    "train_data['tokens_raw'] = train_data['OriginalTweet'].apply(lambda x: tk.tokenize(x.lower()))\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d85fcce6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T23:08:39.664384Z",
     "iopub.status.busy": "2022-11-17T23:08:39.664004Z",
     "iopub.status.idle": "2022-11-17T23:08:40.536154Z",
     "shell.execute_reply": "2022-11-17T23:08:40.535033Z"
    },
    "papermill": {
     "duration": 0.885183,
     "end_time": "2022-11-17T23:08:40.538714",
     "exception": false,
     "start_time": "2022-11-17T23:08:39.653531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>tokens_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>[trending, :, new, yorkers, encounter, empty, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[when, i, couldn't, find, hand, sanitizer, at,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>[find, out, how, you, can, protect, yourself, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[#panic, buying, hits, #newyork, city, as, anx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[#toiletpaper, #dunnypaper, #coronavirus, #cor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment  \\\n",
       "0  TRENDING: New Yorkers encounter empty supermar...  Extremely Negative   \n",
       "1  When I couldn't find hand sanitizer at Fred Me...            Positive   \n",
       "2  Find out how you can protect yourself and love...  Extremely Positive   \n",
       "3  #Panic buying hits #NewYork City as anxious sh...            Negative   \n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...             Neutral   \n",
       "\n",
       "                                          tokens_raw  \n",
       "0  [trending, :, new, yorkers, encounter, empty, ...  \n",
       "1  [when, i, couldn't, find, hand, sanitizer, at,...  \n",
       "2  [find, out, how, you, can, protect, yourself, ...  \n",
       "3  [#panic, buying, hits, #newyork, city, as, anx...  \n",
       "4  [#toiletpaper, #dunnypaper, #coronavirus, #cor...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['tokens_raw'] = test_data['OriginalTweet'].apply(lambda x: tk.tokenize(x.lower()))\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9023bf41",
   "metadata": {
    "papermill": {
     "duration": 0.008968,
     "end_time": "2022-11-17T23:08:40.557082",
     "exception": false,
     "start_time": "2022-11-17T23:08:40.548114",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This takes out any unnecessary token words that we don't want in our dataset such as \"the\", \"to\", \"I\", or any punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c113741",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T23:08:40.577283Z",
     "iopub.status.busy": "2022-11-17T23:08:40.576872Z",
     "iopub.status.idle": "2022-11-17T23:08:42.128277Z",
     "shell.execute_reply": "2022-11-17T23:08:42.127191Z"
    },
    "papermill": {
     "duration": 1.564378,
     "end_time": "2022-11-17T23:08:42.130724",
     "exception": false,
     "start_time": "2022-11-17T23:08:40.566346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>tokens_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[@menyrbie, @phil_gahan, @chrisitv, https://t....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[advice, talk, neighbours, family, exchange, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[coronavirus, australia, woolworths, give, eld...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[food, stock, one, empty, ..., please, ,, pani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>[,, ready, go, supermarket, outbreak, i'm, par...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment  \\\n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral   \n",
       "1  advice Talk to your neighbours family to excha...            Positive   \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive   \n",
       "3  My food stock is not the only one which is emp...            Positive   \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative   \n",
       "\n",
       "                                          tokens_raw  \n",
       "0  [@menyrbie, @phil_gahan, @chrisitv, https://t....  \n",
       "1  [advice, talk, neighbours, family, exchange, p...  \n",
       "2  [coronavirus, australia, woolworths, give, eld...  \n",
       "3  [food, stock, one, empty, ..., please, ,, pani...  \n",
       "4  [,, ready, go, supermarket, outbreak, i'm, par...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "removechars = set(['.','/','!','/','?',':',';'])\n",
    "train_data['tokens_raw'] = train_data['tokens_raw'].apply(lambda x: [w for w in x if w not in stop])\n",
    "train_data['tokens_raw'] = train_data['tokens_raw'].apply(lambda x: [w for w in x if w not in removechars])\n",
    "train_data['tokens_raw'] = train_data['tokens_raw'].apply(lambda x: [w for w in x if not re.match('^#', w)])\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89308121",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T23:08:42.152017Z",
     "iopub.status.busy": "2022-11-17T23:08:42.151614Z",
     "iopub.status.idle": "2022-11-17T23:08:42.309251Z",
     "shell.execute_reply": "2022-11-17T23:08:42.308414Z"
    },
    "papermill": {
     "duration": 0.170971,
     "end_time": "2022-11-17T23:08:42.311449",
     "exception": false,
     "start_time": "2022-11-17T23:08:42.140478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>tokens_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>[trending, new, yorkers, encounter, empty, sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[find, hand, sanitizer, fred, meyer, ,, turned...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>[find, protect, loved, ones]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[buying, hits, city, anxious, shoppers, stock,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[one, week, everyone, buying, baby, milk, powd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment  \\\n",
       "0  TRENDING: New Yorkers encounter empty supermar...  Extremely Negative   \n",
       "1  When I couldn't find hand sanitizer at Fred Me...            Positive   \n",
       "2  Find out how you can protect yourself and love...  Extremely Positive   \n",
       "3  #Panic buying hits #NewYork City as anxious sh...            Negative   \n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...             Neutral   \n",
       "\n",
       "                                          tokens_raw  \n",
       "0  [trending, new, yorkers, encounter, empty, sup...  \n",
       "1  [find, hand, sanitizer, fred, meyer, ,, turned...  \n",
       "2                       [find, protect, loved, ones]  \n",
       "3  [buying, hits, city, anxious, shoppers, stock,...  \n",
       "4  [one, week, everyone, buying, baby, milk, powd...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['tokens_raw'] = test_data['tokens_raw'].apply(lambda x: [w for w in x if w not in stop])\n",
    "test_data['tokens_raw'] = test_data['tokens_raw'].apply(lambda x: [w for w in x if w not in removechars])\n",
    "test_data['tokens_raw'] = test_data['tokens_raw'].apply(lambda x: [w for w in x if not re.match('^#', w)])\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9e1d58",
   "metadata": {
    "papermill": {
     "duration": 0.009495,
     "end_time": "2022-11-17T23:08:42.330929",
     "exception": false,
     "start_time": "2022-11-17T23:08:42.321434",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Instead of having the same words in different forms have their own token value, we group all of those words together to be only one token value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d5a3967",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T23:08:42.352602Z",
     "iopub.status.busy": "2022-11-17T23:08:42.351852Z",
     "iopub.status.idle": "2022-11-17T23:08:49.160492Z",
     "shell.execute_reply": "2022-11-17T23:08:49.158611Z"
    },
    "papermill": {
     "duration": 6.822827,
     "end_time": "2022-11-17T23:08:49.163568",
     "exception": false,
     "start_time": "2022-11-17T23:08:42.340741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>tokens_raw</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[@menyrbie, @phil_gahan, @chrisitv, https://t....</td>\n",
       "      <td>[@menyrbie, @phil_gahan, @chrisitv, https://t....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[advice, talk, neighbours, family, exchange, p...</td>\n",
       "      <td>[advice, talk, neighbour, family, exchange, ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[coronavirus, australia, woolworths, give, eld...</td>\n",
       "      <td>[coronavirus, australia, woolworths, give, eld...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[food, stock, one, empty, ..., please, ,, pani...</td>\n",
       "      <td>[food, stock, one, empty, ..., please, ,, pani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>[,, ready, go, supermarket, outbreak, i'm, par...</td>\n",
       "      <td>[,, ready, go, supermarket, outbreak, i'm, par...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment  \\\n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral   \n",
       "1  advice Talk to your neighbours family to excha...            Positive   \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive   \n",
       "3  My food stock is not the only one which is emp...            Positive   \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative   \n",
       "\n",
       "                                          tokens_raw  \\\n",
       "0  [@menyrbie, @phil_gahan, @chrisitv, https://t....   \n",
       "1  [advice, talk, neighbours, family, exchange, p...   \n",
       "2  [coronavirus, australia, woolworths, give, eld...   \n",
       "3  [food, stock, one, empty, ..., please, ,, pani...   \n",
       "4  [,, ready, go, supermarket, outbreak, i'm, par...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [@menyrbie, @phil_gahan, @chrisitv, https://t....  \n",
       "1  [advice, talk, neighbour, family, exchange, ph...  \n",
       "2  [coronavirus, australia, woolworths, give, eld...  \n",
       "3  [food, stock, one, empty, ..., please, ,, pani...  \n",
       "4  [,, ready, go, supermarket, outbreak, i'm, par...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "train_data['tokens'] = train_data['tokens_raw'].apply(lambda x: [lemmatizer.lemmatize(w, pos=\"v\") for w in x])\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af209b24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T23:08:49.186563Z",
     "iopub.status.busy": "2022-11-17T23:08:49.186105Z",
     "iopub.status.idle": "2022-11-17T23:08:49.603782Z",
     "shell.execute_reply": "2022-11-17T23:08:49.602578Z"
    },
    "papermill": {
     "duration": 0.431594,
     "end_time": "2022-11-17T23:08:49.606260",
     "exception": false,
     "start_time": "2022-11-17T23:08:49.174666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>tokens_raw</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>[trending, new, yorkers, encounter, empty, sup...</td>\n",
       "      <td>[trend, new, yorkers, encounter, empty, superm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[find, hand, sanitizer, fred, meyer, ,, turned...</td>\n",
       "      <td>[find, hand, sanitizer, fred, meyer, ,, turn, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "      <td>[find, protect, loved, ones]</td>\n",
       "      <td>[find, protect, love, ones]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[buying, hits, city, anxious, shoppers, stock,...</td>\n",
       "      <td>[buy, hit, city, anxious, shoppers, stock, foo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[one, week, everyone, buying, baby, milk, powd...</td>\n",
       "      <td>[one, week, everyone, buy, baby, milk, powder,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment  \\\n",
       "0  TRENDING: New Yorkers encounter empty supermar...  Extremely Negative   \n",
       "1  When I couldn't find hand sanitizer at Fred Me...            Positive   \n",
       "2  Find out how you can protect yourself and love...  Extremely Positive   \n",
       "3  #Panic buying hits #NewYork City as anxious sh...            Negative   \n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...             Neutral   \n",
       "\n",
       "                                          tokens_raw  \\\n",
       "0  [trending, new, yorkers, encounter, empty, sup...   \n",
       "1  [find, hand, sanitizer, fred, meyer, ,, turned...   \n",
       "2                       [find, protect, loved, ones]   \n",
       "3  [buying, hits, city, anxious, shoppers, stock,...   \n",
       "4  [one, week, everyone, buying, baby, milk, powd...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [trend, new, yorkers, encounter, empty, superm...  \n",
       "1  [find, hand, sanitizer, fred, meyer, ,, turn, ...  \n",
       "2                        [find, protect, love, ones]  \n",
       "3  [buy, hit, city, anxious, shoppers, stock, foo...  \n",
       "4  [one, week, everyone, buy, baby, milk, powder,...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['tokens'] = test_data['tokens_raw'].apply(lambda x: [lemmatizer.lemmatize(w, pos=\"v\") for w in x])\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babf3e67",
   "metadata": {
    "papermill": {
     "duration": 0.010301,
     "end_time": "2022-11-17T23:08:49.627151",
     "exception": false,
     "start_time": "2022-11-17T23:08:49.616850",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Make a vocabulary for all the words that are in our training data set, and we keep track how much they appear all together, and how much they appear in each tweet that is positive, negative, extremely positive, extremely negative, and neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c869f7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T23:08:49.650087Z",
     "iopub.status.busy": "2022-11-17T23:08:49.649428Z",
     "iopub.status.idle": "2022-11-17T23:08:50.219987Z",
     "shell.execute_reply": "2022-11-17T23:08:50.218499Z"
    },
    "papermill": {
     "duration": 0.58476,
     "end_time": "2022-11-17T23:08:50.222296",
     "exception": false,
     "start_time": "2022-11-17T23:08:49.637536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tokens: 67595, Pos: 26716, Ex Pos: 18154, Neg: 23488, Ex Neg: 15277, Neu: 19492\n"
     ]
    }
   ],
   "source": [
    "train_pos = train_data[train_data['Sentiment'] == 'Positive']\n",
    "train_expos = train_data[train_data['Sentiment'] == 'Extremely Positive']\n",
    "train_neg = train_data[train_data['Sentiment'] == 'Negative']\n",
    "train_exneg = train_data[train_data['Sentiment'] == 'Extremely Negative']\n",
    "train_neu = train_data[train_data['Sentiment'] == 'Neutral']\n",
    "\n",
    "def vocab_list(tokens_column):\n",
    "    vocab = dict()\n",
    "    for tweet_tokens in tokens_column:\n",
    "        for token in tweet_tokens:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = 1\n",
    "            else:\n",
    "                vocab[token] += 1\n",
    "    return vocab\n",
    "\n",
    "vocab_all = dict(sorted(vocab_list(train_data['tokens']).items(), key=lambda item: item[1], reverse=True))\n",
    "vocab_pos = dict(sorted(vocab_list(train_pos['tokens']).items(), key=lambda item: item[1], reverse=True))\n",
    "vocab_expos = dict(sorted(vocab_list(train_expos['tokens']).items(), key=lambda item: item[1], reverse=True))\n",
    "vocab_neg = dict(sorted(vocab_list(train_neg['tokens']).items(), key=lambda item: item[1], reverse=True))\n",
    "vocab_exneg = dict(sorted(vocab_list(train_exneg['tokens']).items(), key=lambda item: item[1], reverse=True))\n",
    "vocab_neu = dict(sorted(vocab_list(train_neu['tokens']).items(), key=lambda item: item[1], reverse=True))\n",
    "print(f\"All tokens: {len(vocab_all)}, Pos: {len(vocab_pos)}, Ex Pos: {len(vocab_expos)}, Neg: {len(vocab_neg)}, Ex Neg: {len(vocab_exneg)}, Neu: {len(vocab_neu)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a90a2cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T23:08:50.245140Z",
     "iopub.status.busy": "2022-11-17T23:08:50.244762Z",
     "iopub.status.idle": "2022-11-17T23:08:50.269521Z",
     "shell.execute_reply": "2022-11-17T23:08:50.268408Z"
    },
    "papermill": {
     "duration": 0.039008,
     "end_time": "2022-11-17T23:08:50.271906",
     "exception": false,
     "start_time": "2022-11-17T23:08:50.232898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 28642),\n",
       " ('19', 12249),\n",
       " ('-', 11511),\n",
       " ('covid', 10607),\n",
       " ('price', 8957),\n",
       " ('store', 8161),\n",
       " ('\\x92', 7155),\n",
       " ('food', 6826),\n",
       " ('supermarket', 6686),\n",
       " ('grocery', 6087),\n",
       " ('people', 5528),\n",
       " ('go', 5332),\n",
       " ('shop', 4875),\n",
       " ('get', 4467),\n",
       " ('consumer', 4274),\n",
       " ('&', 3837),\n",
       " ('need', 3546),\n",
       " ('\"', 3404),\n",
       " ('buy', 3370),\n",
       " ('â', 3358)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab_all.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a49cab0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T23:08:50.295522Z",
     "iopub.status.busy": "2022-11-17T23:08:50.295131Z",
     "iopub.status.idle": "2022-11-17T23:08:50.304159Z",
     "shell.execute_reply": "2022-11-17T23:08:50.303351Z"
    },
    "papermill": {
     "duration": 0.023462,
     "end_time": "2022-11-17T23:08:50.306069",
     "exception": false,
     "start_time": "2022-11-17T23:08:50.282607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_n_to_remove = 1000\n",
    "for i, item in enumerate(vocab_all.items()):\n",
    "    if i == top_n_to_remove:\n",
    "        break\n",
    "    if item[0] in vocab_pos:\n",
    "        del vocab_pos[item[0]]\n",
    "    if item[0] in vocab_expos:\n",
    "        del vocab_expos[item[0]]\n",
    "    if item[0] in vocab_neg:\n",
    "        del vocab_neg[item[0]]\n",
    "    if item[0] in vocab_exneg:\n",
    "        del vocab_exneg[item[0]]\n",
    "    if item[0] in vocab_neu:\n",
    "        del vocab_neu[item[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f87ca918",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T23:08:50.329632Z",
     "iopub.status.busy": "2022-11-17T23:08:50.328514Z",
     "iopub.status.idle": "2022-11-17T23:08:50.341961Z",
     "shell.execute_reply": "2022-11-17T23:08:50.340851Z"
    },
    "papermill": {
     "duration": 0.027828,
     "end_time": "2022-11-17T23:08:50.344595",
     "exception": false,
     "start_time": "2022-11-17T23:08:50.316767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('novel', 59),\n",
       " ('greater', 49),\n",
       " ('epidemic', 48),\n",
       " ('behaviors', 46),\n",
       " ('distribute', 45),\n",
       " ('fair', 44),\n",
       " ('surface', 44),\n",
       " ('significant', 43),\n",
       " ('page', 42),\n",
       " ('highlight', 41),\n",
       " ('approve', 41),\n",
       " ('industries', 41),\n",
       " ('hop', 41),\n",
       " ('traffic', 41),\n",
       " ('recently', 40),\n",
       " ('aware', 39),\n",
       " ('teachers', 39),\n",
       " ('husband', 39),\n",
       " ('dm', 39),\n",
       " ('sanitiser', 38)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab_pos.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a040313",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T23:08:50.368333Z",
     "iopub.status.busy": "2022-11-17T23:08:50.367556Z",
     "iopub.status.idle": "2022-11-17T23:08:50.380785Z",
     "shell.execute_reply": "2022-11-17T23:08:50.379596Z"
    },
    "papermill": {
     "duration": 0.027585,
     "end_time": "2022-11-17T23:08:50.382901",
     "exception": false,
     "start_time": "2022-11-17T23:08:50.355316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('grateful', 65),\n",
       " ('super', 59),\n",
       " ('win', 50),\n",
       " ('enjoy', 50),\n",
       " ('trust', 50),\n",
       " ('fun', 46),\n",
       " ('safely', 45),\n",
       " ('lol', 43),\n",
       " ('smart', 43),\n",
       " ('proud', 42),\n",
       " ('bless', 42),\n",
       " ('kindly', 41),\n",
       " ('truly', 41),\n",
       " ('rt', 40),\n",
       " ('professionals', 40),\n",
       " ('helpful', 40),\n",
       " ('solution', 39),\n",
       " ('smile', 39),\n",
       " ('greater', 38),\n",
       " ('commit', 38)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab_expos.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8dc2397",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T23:08:50.406658Z",
     "iopub.status.busy": "2022-11-17T23:08:50.405902Z",
     "iopub.status.idle": "2022-11-17T23:08:50.419343Z",
     "shell.execute_reply": "2022-11-17T23:08:50.418113Z"
    },
    "papermill": {
     "duration": 0.027953,
     "end_time": "2022-11-17T23:08:50.421758",
     "exception": false,
     "start_time": "2022-11-17T23:08:50.393805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('uncertainty', 53),\n",
       " ('damn', 50),\n",
       " ('export', 47),\n",
       " ('lowest', 46),\n",
       " ('file', 44),\n",
       " ('problems', 43),\n",
       " ('fell', 42),\n",
       " ('officials', 41),\n",
       " ('arabia', 40),\n",
       " ('complete', 39),\n",
       " ('strain', 38),\n",
       " ('threat', 38),\n",
       " ('inflation', 37),\n",
       " ('suspend', 37),\n",
       " ('barrel', 37),\n",
       " ('dairy', 37),\n",
       " ('hereâ', 36),\n",
       " ('chicken', 36),\n",
       " ('output', 36),\n",
       " ('largest', 36)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab_neg.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9ad325b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T23:08:50.445965Z",
     "iopub.status.busy": "2022-11-17T23:08:50.445498Z",
     "iopub.status.idle": "2022-11-17T23:08:50.457028Z",
     "shell.execute_reply": "2022-11-17T23:08:50.456086Z"
    },
    "papermill": {
     "duration": 0.026192,
     "end_time": "2022-11-17T23:08:50.459146",
     "exception": false,
     "start_time": "2022-11-17T23:08:50.432954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fraud', 68),\n",
       " ('greedy', 63),\n",
       " ('hell', 62),\n",
       " ('stupid', 60),\n",
       " ('attack', 59),\n",
       " ('steal', 59),\n",
       " ('disgust', 55),\n",
       " ('sad', 55),\n",
       " ('exploit', 50),\n",
       " ('destroy', 49),\n",
       " ('threat', 47),\n",
       " ('negative', 46),\n",
       " ('cry', 45),\n",
       " ('ass', 44),\n",
       " ('dead', 43),\n",
       " ('wtf', 43),\n",
       " ('lowest', 43),\n",
       " ('hunger', 43),\n",
       " ('chaos', 42),\n",
       " ('hurt', 42)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab_exneg.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d5e65f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T23:08:50.483564Z",
     "iopub.status.busy": "2022-11-17T23:08:50.482963Z",
     "iopub.status.idle": "2022-11-17T23:08:50.494112Z",
     "shell.execute_reply": "2022-11-17T23:08:50.493026Z"
    },
    "papermill": {
     "duration": 0.026308,
     "end_time": "2022-11-17T23:08:50.496639",
     "exception": false,
     "start_time": "2022-11-17T23:08:50.470331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_', 54),\n",
       " ('property', 36),\n",
       " ('california', 32),\n",
       " ('@youtube', 30),\n",
       " ('ticket', 29),\n",
       " ('jump', 29),\n",
       " ('11', 28),\n",
       " ('©', 27),\n",
       " ('guidance', 27),\n",
       " ('reveal', 27),\n",
       " ('series', 26),\n",
       " ('arrive', 26),\n",
       " ('platform', 26),\n",
       " ('de', 26),\n",
       " ('¼', 26),\n",
       " ('locations', 25),\n",
       " ('mark', 25),\n",
       " ('boom', 25),\n",
       " ('mall', 25),\n",
       " ('friday', 24)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab_neu.items())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c00746",
   "metadata": {
    "papermill": {
     "duration": 0.011035,
     "end_time": "2022-11-17T23:08:50.519190",
     "exception": false,
     "start_time": "2022-11-17T23:08:50.508155",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Change the from of the observation from tokens into a string so we can use the vectorizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54d672bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T23:08:50.543369Z",
     "iopub.status.busy": "2022-11-17T23:08:50.542942Z",
     "iopub.status.idle": "2022-11-17T23:08:50.602053Z",
     "shell.execute_reply": "2022-11-17T23:08:50.600942Z"
    },
    "papermill": {
     "duration": 0.073953,
     "end_time": "2022-11-17T23:08:50.604498",
     "exception": false,
     "start_time": "2022-11-17T23:08:50.530545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>tokens_raw</th>\n",
       "      <th>tokens</th>\n",
       "      <th>textclean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[@menyrbie, @phil_gahan, @chrisitv, https://t....</td>\n",
       "      <td>[@menyrbie, @phil_gahan, @chrisitv, https://t....</td>\n",
       "      <td>@menyrbie @phil_gahan @chrisitv https://t.co/i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[advice, talk, neighbours, family, exchange, p...</td>\n",
       "      <td>[advice, talk, neighbour, family, exchange, ph...</td>\n",
       "      <td>advice talk neighbour family exchange phone nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[coronavirus, australia, woolworths, give, eld...</td>\n",
       "      <td>[coronavirus, australia, woolworths, give, eld...</td>\n",
       "      <td>coronavirus australia woolworths give elderly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[food, stock, one, empty, ..., please, ,, pani...</td>\n",
       "      <td>[food, stock, one, empty, ..., please, ,, pani...</td>\n",
       "      <td>food stock one empty ... please , panic , enou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>[,, ready, go, supermarket, outbreak, i'm, par...</td>\n",
       "      <td>[,, ready, go, supermarket, outbreak, i'm, par...</td>\n",
       "      <td>, ready go supermarket outbreak i'm paranoid ,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet           Sentiment  \\\n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral   \n",
       "1  advice Talk to your neighbours family to excha...            Positive   \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive   \n",
       "3  My food stock is not the only one which is emp...            Positive   \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative   \n",
       "\n",
       "                                          tokens_raw  \\\n",
       "0  [@menyrbie, @phil_gahan, @chrisitv, https://t....   \n",
       "1  [advice, talk, neighbours, family, exchange, p...   \n",
       "2  [coronavirus, australia, woolworths, give, eld...   \n",
       "3  [food, stock, one, empty, ..., please, ,, pani...   \n",
       "4  [,, ready, go, supermarket, outbreak, i'm, par...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [@menyrbie, @phil_gahan, @chrisitv, https://t....   \n",
       "1  [advice, talk, neighbour, family, exchange, ph...   \n",
       "2  [coronavirus, australia, woolworths, give, eld...   \n",
       "3  [food, stock, one, empty, ..., please, ,, pani...   \n",
       "4  [,, ready, go, supermarket, outbreak, i'm, par...   \n",
       "\n",
       "                                           textclean  \n",
       "0  @menyrbie @phil_gahan @chrisitv https://t.co/i...  \n",
       "1  advice talk neighbour family exchange phone nu...  \n",
       "2  coronavirus australia woolworths give elderly ...  \n",
       "3  food stock one empty ... please , panic , enou...  \n",
       "4  , ready go supermarket outbreak i'm paranoid ,...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['textclean'] = train_data['tokens'].apply(lambda x: ' '.join(x))\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0535bdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T23:08:50.629431Z",
     "iopub.status.busy": "2022-11-17T23:08:50.628802Z",
     "iopub.status.idle": "2022-11-17T23:08:55.806418Z",
     "shell.execute_reply": "2022-11-17T23:08:55.805147Z"
    },
    "papermill": {
     "duration": 5.19273,
     "end_time": "2022-11-17T23:08:55.808722",
     "exception": false,
     "start_time": "2022-11-17T23:08:50.615992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (41157, 64082)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train = tfidf_vectorizer.fit_transform(train_data['textclean']).toarray()\n",
    "print(f\"X_train.shape = {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39ad2dd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T23:08:55.833764Z",
     "iopub.status.busy": "2022-11-17T23:08:55.832817Z",
     "iopub.status.idle": "2022-11-17T23:08:55.840207Z",
     "shell.execute_reply": "2022-11-17T23:08:55.839169Z"
    },
    "papermill": {
     "duration": 0.022412,
     "end_time": "2022-11-17T23:08:55.842618",
     "exception": false,
     "start_time": "2022-11-17T23:08:55.820206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Neutral', 'Positive', 'Positive', 'Positive',\n",
       "       'Extremely Negative', 'Positive', 'Positive', 'Neutral',\n",
       "       'Positive', 'Negative'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train_data.Sentiment.to_numpy()\n",
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cab103",
   "metadata": {
    "papermill": {
     "duration": 0.011132,
     "end_time": "2022-11-17T23:08:55.865338",
     "exception": false,
     "start_time": "2022-11-17T23:08:55.854206",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This currently doesn't work. It's supposed to create a confusion matrix using the random forest model but it seems that it never completes running. So will need to do some debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68892d69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T23:08:55.890362Z",
     "iopub.status.busy": "2022-11-17T23:08:55.889701Z",
     "iopub.status.idle": "2022-11-17T23:08:55.894658Z",
     "shell.execute_reply": "2022-11-17T23:08:55.893502Z"
    },
    "papermill": {
     "duration": 0.019848,
     "end_time": "2022-11-17T23:08:55.896695",
     "exception": false,
     "start_time": "2022-11-17T23:08:55.876847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "#model = RandomForestClassifier(n_estimators = 100)\n",
    "#model = model.fit(X_train, y_train)\n",
    "\n",
    "#pred_train = model.predict(X_train)\n",
    "\n",
    "#display = ConfusionMatrixDisplay(confusion_matrix(train_data['Sentiment'], pred_train))\n",
    "#display.plot()\n",
    "#print(f\"accuracy: {accuracy_score(train_data['Sentiment'],pred_train):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c3bb0e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T23:08:55.921685Z",
     "iopub.status.busy": "2022-11-17T23:08:55.920949Z",
     "iopub.status.idle": "2022-11-17T23:08:56.389300Z",
     "shell.execute_reply": "2022-11-17T23:08:56.388137Z"
    },
    "papermill": {
     "duration": 0.483503,
     "end_time": "2022-11-17T23:08:56.391689",
     "exception": false,
     "start_time": "2022-11-17T23:08:55.908186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3798, 64082)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['textclean'] = test_data['tokens'].apply(lambda x: ' '.join(x))\n",
    "X_test = tfidf_vectorizer.transform(test_data['textclean']).toarray()\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "315eebdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T23:08:56.417231Z",
     "iopub.status.busy": "2022-11-17T23:08:56.416448Z",
     "iopub.status.idle": "2022-11-17T23:08:56.421033Z",
     "shell.execute_reply": "2022-11-17T23:08:56.420330Z"
    },
    "papermill": {
     "duration": 0.01985,
     "end_time": "2022-11-17T23:08:56.423194",
     "exception": false,
     "start_time": "2022-11-17T23:08:56.403344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pred_test = model.predict(X_test)\n",
    "#display = ConfusionMatrixDisplay(confusion_matrix(test_data['Sentiment'], pred_train))\n",
    "#display.plot()\n",
    "#print(f\"accuracy: {accuracy_score(test_data['Sentiment'],pred_test):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 35.844399,
   "end_time": "2022-11-17T23:08:57.759047",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-17T23:08:21.914648",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
